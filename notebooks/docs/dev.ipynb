{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47fbc2d-28c1-4284-a63c-aa85ac29e617",
   "metadata": {},
   "source": [
    "# Giga Dev Documentation\n",
    "\n",
    "This documentation provides technical details on the Giga model's implementation,\n",
    "setup, and deployment process.\n",
    "\n",
    "> Also see the following additional documentation:\n",
    "> * [User overview](main.ipynb) and details on running each notebook.\n",
    "> * [Model overview](models.ipynb), including a breakdown of each model.\n",
    "> * [Model data](data.ipynb), including data schemas and how to update countries.\n",
    "> * [Model architecture](arch.ipynb), focusing on key parts of the library used for configuration, data aggregation, and model execution.\n",
    "> * [Python documentation](../dev/documentation.ipynb) automatically generated from the model source code.\n",
    "\n",
    "### Repository Structure\n",
    "\n",
    "The python library in this repository is organized into the following key categories to help manage the models and their parameters:\n",
    "\n",
    "1. Models: the key building blocks of all computations performed by this library\n",
    "2. Schemas: the definitions of all the model inputs and outputs, data requirements, and configurations\n",
    "3. Data: the tooling to pull in and transform any external data into formats usable by the library\n",
    "4. Utilities: helpers for connecting to APIs, visualizing outputs, and constructing inspect able and interactive interfaces\n",
    "5. App: the application runner for configuring and starting the modeling application\n",
    "\n",
    "## Setup\n",
    "\n",
    "Note: this repositroy uses git lfs for some of the larger files.\n",
    "Please install [git lfs](https://git-lfs.com/), and then run `git lfs pull` to fetch copies of the larger files locally.\n",
    "Use [poetry](https://python-poetry.org/) to create a local development environment.\n",
    "Poetry is a tool for dependency management in Python.\n",
    "You can use the helper `dev` CLI to build the environment locally:\n",
    "\n",
    "```bash\n",
    "./dev build\n",
    "```\n",
    "\n",
    "To start a local notebook server simply run:\n",
    "\n",
    "```bash\n",
    "./dev start-notebook\n",
    "```\n",
    "\n",
    "You can use the `dev` CLI to also run pytest tests:\n",
    "\n",
    "```bash\n",
    "./dev test\n",
    "```\n",
    "\n",
    "### Lint\n",
    "\n",
    "You can format local code using the following commands:\n",
    "\n",
    "```bash\n",
    "./dev lint    # Runs flake8 link check against PEP8 standard\n",
    "./dev format  # Auto-formats code that is non PEP8-compliant\n",
    "```\n",
    "\n",
    "\n",
    "## Deployment\n",
    "\n",
    "To build the model container and re-deploy the notebook cluster simply run:\n",
    "\n",
    "```bash\n",
    "./stack up\n",
    "```\n",
    "\n",
    "To stop the cluster and clear resources run:\n",
    "\n",
    "```bash\n",
    "./stack down\n",
    "```\n",
    "\n",
    "Please note, you will need to have authenticated with GCP CLI and have k8s context referencing the right GKE cluster. \n",
    "For more details on this see below. \n",
    "\n",
    "### Cluster Details\n",
    "\n",
    "Notebooks are deployed as a standalone application using [JupyterHub](https://jupyter.org/hub).\n",
    "These notebooks allow users to interact with the giga models through an interactive dashboard and to visualize/plot the model outputs through a streamlined interfaces.\n",
    "\n",
    "[Helm](https://helm.sh/) is used to manage the deployment - find the existing jupyterhub helm chart [here](https://artifacthub.io/packages/helm/jupyterhub/jupyterhub).\n",
    "The deployment configuration for this chart can be found in `deployment/values/prod.yaml`.\n",
    "The following configurations are managed with a custom configuration:\n",
    "1. The base notebook container used in the deployment that includes the models\n",
    "2. The authentication mechanism for users to access jupyterhub - auth0 is currently used\n",
    "\n",
    "### Resource Requirements\n",
    "\n",
    "The model application is typically memory constrained rather than CPU constrained.\n",
    "The recommended minimum memory for a single model pod in k8s is 3 GB with a limit 5 GB.\n",
    "The current configuration is set to reflect this as follows (from deployment/help/prod.yaml):\n",
    "\n",
    "```\n",
    "singleuser:\n",
    "  # other configs ...\n",
    "  cpu:\n",
    "    limit:\n",
    "    guarantee:\n",
    "  memory:\n",
    "    limit: 5G\n",
    "    guarantee: 3G\n",
    "```\n",
    "\n",
    "Do note that no cpu limit is not set above.\n",
    "Additionally, the somewhat large memory guarantee is needed to run models for all schools in large countries (like Brazil).\n",
    "If the school data is broken down into smaller sub-regions for those larger countries, it's likely possible to make the memory guarantee significantly smaller.\n",
    "\n",
    "### Deployment Workflow\n",
    "\n",
    "Please note that the workflow is currently manually managed with the CLI explained below.\n",
    "The full deployment workflow looks as follows, which can all be managed with the `stack` CLI: \n",
    "1. Authenticate with GCP by running `./stack auth`. This will also configure the credentials for the GKE cluster to which jupyterhub is deployed\n",
    "2. Create a Docker image for the models, you can use the CLI in the root dir: `./stack create-image`\n",
    "3. Push the image to Actual's docker registry: `./stack push-image`\n",
    "4. Update or launch a new instance of the cluster with `./stack launch` \n",
    "\n",
    "> **Note** In order to authenticate with external services, [environment secrets](#environment-secrets) must be injected during deployment.\n",
    "\n",
    "### Updating the Cluster + Local Testing\n",
    "\n",
    "You can stop the jupyterhub cluster by running `./stack stop`.\n",
    "If you need to update the single user image, you can rebuild it using the CLI above.\n",
    "You can interact with the single user container locally by running `./stack start-container <local-workspace>`.\n",
    "\n",
    "### Environment Secrets\n",
    "\n",
    "The deployed application authenticates with a number of backends via deployment and\n",
    "environment secrets that are not checked in with the application code. \n",
    "\n",
    "| Secret | Format | Usage |\n",
    "| --- | --- | --- |\n",
    "| `MAP_BOX_ACCESS_TOKEN` | MapBox API access token string | Used to display detailed country maps during school selection and results visualization. |\n",
    "| `OBJSTORE_GCS_CREDS` | JSON service account credentials | Used to connect to Google Cloud APIs, primarily object store |\n",
    "| `GIGA_AUTH_TOKEN` | Bearer token to Giga Connect API | Used to fetch updated |\n",
    "\n",
    "---\n",
    "\n",
    "When deploying the application, you can use deployment secrets to inject these environment\n",
    "variables, or populate them in an `.env` file in the root folder.\n",
    "\n",
    "### Authentication and Authorization Configuration\n",
    "\n",
    "You can configure the application cluster to use a number of different authenticators, these include:\n",
    "\n",
    "* github\n",
    "* Azure Active Directory\n",
    "* Auth0\n",
    "* Google auth\n",
    "\n",
    "You can read more about configuring each of these [here](https://z2jh.jupyter.org/en/stable/administrator/authentication.html).\n",
    "\n",
    "To configure an authenticator you will need to update the helm values in deployment/help/prod.yaml under\n",
    "\n",
    "```\n",
    "hub:\n",
    "  config:\n",
    "    # authenticator configuration here, for auth0 see example below\n",
    "    Auth0OAuthenticator:\n",
    "      client_id: client-id-from-auth0-here\n",
    "      client_secret: client-secret-from-auth0-here\n",
    "      oauth_callback_url: https://your-jupyterhub-domain/hub/oauth_callback\n",
    "      scope:\n",
    "        - openid\n",
    "        - email\n",
    "      auth0_subdomain: prod-8ua-1yy9\n",
    "    Authenticator:\n",
    "      admin_users:\n",
    "        - devops@example.com\n",
    "      auto_login: true\n",
    "    JupyterHub:\n",
    "      authenticator_class: auth0\n",
    "```\n",
    "\n",
    "### GCP and Auth0 Configurations\n",
    "\n",
    "Configuring the deployment is done in two places, the `stack` CLI and the deployment manifest of helm values.\n",
    "Most of the GCP specific deployment parameters are defined in the stack CLI, the ones of interest are the following:\n",
    "\n",
    "* The container registry, which is where all the built docker containers are pushed to and pulled from, see [here](stack#L6)\n",
    "* The cluster name, which points to the k8s cluster running the deployment, see [here](stack#L12)\n",
    "* The auth configuration is managed entirely inside of our deployment manifest, see [here](deployment/helm/prod.yaml#L31)\n",
    "\n",
    "Migrating to a different cloud provider or a different auth system would require updating these parameters.\n",
    "\n",
    "### Managing Voila Dashboards\n",
    "\n",
    "Once the application is deployed you can create a standalone voila dashboard on a stand-alone url.\n",
    "Currently, dashboard can not be shared between different accounts.\n",
    "One way to have demo-able and sharable dashboards is to create a guest credential, create a dashboard for the application user associated with that credential, and sharing that credential and url with the user who needs access to the dashboard.\n",
    "The standalone dashboard can support multi-tenantcy (e.g. multiple accounts logged in at the same time).\n",
    "However, the dashboard is running in a single pod that is constrained by the deployed resources associated with a single-user pod.\n",
    "\n",
    "To create a dashboard follow the steps below:\n",
    "1. Log-in using the credential you want to provide dashboard access for (this could be your own personal account, a guest credential, or something else)\n",
    "2. Navigate to the hub control panel - click `File` (top right) and `Hub Control Panel` (at the bottom of the menu)\n",
    "3. Click on the `Dashboards` sub-menu at the top\n",
    "4. Click `New Dashboard` to begin creating a new dashboard\n",
    "5. Fill in the information for the dashboard you want to create by specifying the dashboard name (which will be reflected in the url), the description of the dashboard which will appear when the dashboard is first started, the framework - choose voila, and the relative path to the notebook that will be turned into a dashboard (see below for an example models dashboard)\n",
    "6. Click `Save` which will create a container in which the dashboard runs\n",
    "\n",
    "**Please note**: that on new deployments or releases the dashboard container will be stopped, and the dashboard will need to be recreated by following the steps above.\n",
    "\n",
    "The properties of a dashboard (step 5) above are as follows:\n",
    "* `Dashboard name` refers to the name of the dashboard and constructs the url under which the dashboard can be accessed. For example, if you call the dashboard `models`, then the user under which the dashboard was created can access the dashboard directly under https://<base-url>/hub/dashboards/models, for a given base url, the dashboard could be accessed directly by at https://giga.notebooks.actualhq.com/hub/dashboards/\n",
    "* `Description` will be the brief text that shows up when the dashboard is starting. If anyone is accessing the dashboard through the url above directly, this property is not surfaced\n",
    "* `Frameworks` specifies the dashboarding framework to use, for this application select `voila`\n",
    "* `Relative Path` refers to the notebook in the single user container that will be used to create the dashboard. The primary notebook for this application can be found under `notebooks/cost-scenario.ipynb`\n",
    "\n",
    "You can create a number of dashboard using different notebooks by following the steps above.\n",
    "\n",
    "### Known Issues\n",
    "\n",
    "#### SSL Certs\n",
    "\n",
    "If you are spinning up a Jupyterhub cluster with a new SSL certificate (e.g. proxy.https.enabled set to true), you may run into a race condition when deploying to GKE clusters.\n",
    "The race condition will results in the `autohttps` pods in the cluster producing timeout logs and you will not be able to access the https instance of the deployment.\n",
    "You should be able to resolve this by re-configuring the autohttps deployment to sleep prior to starting up.\n",
    "To do so:\n",
    "\n",
    "```bash\n",
    "kubectl edit deploy autohttps\n",
    "```\n",
    "\n",
    "```\n",
    "       # ...\n",
    "       containers:\n",
    "       - image: traefik:v2.6.1\n",
    "+        command: [\"sh\", \"-c\", \"sleep 10 && /entrypoint.sh traefik\"] # add this line\n",
    "         imagePullPolicy: IfNotPresent\n",
    "         # ...\n",
    "```\n",
    "This will trigger the deployment to restart and should resolve the issue.\n",
    "You can find a github issue that explains this in detail [here](https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/2601).\n",
    "Please note you only need to do this once for a new deployment and are using SSL in cases where an SSL certificate doesn't yet exist.\n",
    "\n",
    "#### Standalone Dashboards\n",
    "\n",
    "In order to create and share voila dashboards you must lock the following dependency to an older version:\n",
    "\n",
    "```\n",
    "simpervisor = \"0.4\"\n",
    "```\n",
    "\n",
    "This is already done in `pyproject.toml`. If you upgrade `simpervisor` to a newer version, it is likely that the stand-alone dashboards will not work.\n",
    "Note that you will still be able to use jupyterhub + voila from inside juypterhub as expected, only the standalone dashboard will be impacted.\n",
    "\n",
    "\n",
    "## CLI\n",
    "\n",
    "The library exposes the following CLI, each with a different purpose.\n",
    "\n",
    "For local development, the `./dev` CLI can be used with the following sub-commands:\n",
    "\n",
    "```\n",
    "  build\t\t\t\t\tBuilds the modeling environment locally\n",
    "  start-notebook\t\t        Start a jupyterlab notebook server locally\n",
    "  test\t\t\t\t\tRuns the unit test suite\n",
    "  lint\t\t\t\t\tRuns a flake8 lint check against PEP 8\n",
    "  format\t\t\t\tModifies non PEP 8 compliant code to be style compliant\n",
    "  clean-notebook <notebook-path> \tRemoves rendered html from jupyter notebooks\n",
    "```\n",
    "\n",
    "For managing deployments, the `./stack` CLI can be used with the following sub-commands:\n",
    "\n",
    "```\n",
    "  up \t\t\t\t\t\tRebuild the modeling environment and deploys the notebook stack to a k8s cluster\n",
    "  down \t\t\t\t\t\tTears down the notebook stack\n",
    "  install \t\t\t\t\tInstall minikube, helm, etc.\n",
    "  auth \t\t\t\t\t\tAuthenticate with GCP\n",
    "  create-image \t\t\t\t\tBuilds docker image for off-platform models\n",
    "  push-image \t\t\t\t\tPushes model docker image to a remote registry\n",
    "  create-hub-image \t\t\t\tBuilds docker image for base jupyterhub service\n",
    "  push-hub-image \t\t\t\tPush jupyterhub docker image to a remote registry\n",
    "  start-container <workspace-dir> \t\tLaunches a Docker container and mounts a workspace directory to it\n",
    "  launch  \t\t\t\t\tLaunches jupyterhub on a kubernetes cluster using helm\n",
    "  stop  \t\t\t\t\tStops the jupyterhub deployment\n",
    "  reset-password  <user-email> \t\t\tSends a password reset email for notebook user\n",
    "```\n",
    "\n",
    "For running the models and relevant data pipelines, the `./run` CLI can be used with the following sub-commands:\n",
    "\n",
    "```\n",
    "  upload-workspace <workspace-dir> \t\t\tCopies the data workspace from the specified target directory to a storage bucket\n",
    "  fetch-workspace <workspace-dir> \t\t\tCopies the data workspace from a storage bucket to the specified target directory\n",
    "```\n",
    "\n",
    "#### (Optional) Execute scenarios directly with Python\n",
    "\n",
    "You can use the script below to run the total cost scenario by doing the following:\n",
    "\n",
    "```bash\n",
    "./total_cost_scenario.py --workspace <path-to-data-workspace>\n",
    "\t\t\t\t\t     --output-file <desired-output-file> # e.g. costs.csv\n",
    "\t\t\t\t\t     --scenario-type minimum-cost # minimum-cost, fiber, cellular, p2p, or satellite\n",
    "```\n",
    "\n",
    "The script above will use the school, fiber, and cellular data in the workspace specified, to create an output .csv table that contains cost information for each school in the input data set.\n",
    "Additionally, you can specify the scenario type by choosing between a `minimum-cost` scenario or a single technology cost scenario (`fiber`, `cellular`, `p2p`, `satellite`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
